---
layout: writing
title: "Postgres: executor bottlenecks"
tags: [writing, pg]
last_updated: 2025-07-08
---
# Postgres: executor bottlenecks

July 08, 2025

As OLAP performance and cloud-native database architectures draw more
attention, it’s worth remembering that the executor remains the core engine
doing the work — row by row, on every worker, even in parallel and distributed
plans.

This post expands on the first slide of two recent talks I gave: one at
[PGConf.in 2025](https://pgconf.in/conferences/pgconfin2025/program/proposals/912)
and another at
[Posette 2025](https://posetteconf.com/2025/talks/hacking-postgres-executor-for-performance/),
both covering executor performance improvements in PostgreSQL. It revisits some
observations about executor bottlenecks and sketches directions for future work
that could realistically improve single-node efficiency without a full executor
rewrite.

## OLTP workloads and executor path

Short OLTP-style queries — such as primary key lookups or targeted updates —
still dominate many production workloads. For example:

```
SELECT * FROM orders WHERE order_id = $1;`
UPDATE accounts SET balance = balance - $1 WHERE account_id = $2;
```

In most cases, executor overhead isn't the dominant cost for these — I/O
latency, WAL flushes, locks, or client round trips often matter more.

And while these queries do traverse the full executor plan tree from root to
leaf to read from storage and back to root to return the tuple to the application,
the tree is usually shallow — just a scan node and maybe a filter or limit. Because
they return after a few tuples, they don’t stress much of the executor’s runtime
machinery, such as join logic, aggregates, or sort nodes.

Still, there are edge cases where executor efficiency becomes noticeable:

- High-frequency queries under tight response time constraints (e.g., <1ms)  
- In-memory workloads on replicas or caches where CPU becomes the main bottleneck  
- Aggregate CPU usage across thousands of QPS — even small inefficiencies add up

So while OLTP isn't the executor’s biggest stress test, it still benefits from
reducing unnecessary work in hot paths like tuple deforming or qual evaluation.

## OLAP is where it piles up

In OLAP workloads, the same inefficiencies don’t just show up — they multiply.
These queries scan millions of tuples, join wide rows, and apply many filters or
aggregates. A few dozen nanoseconds of overhead per tuple can easily add up to
hundreds of milliseconds in total.

Even with parallel or distributed plans, each worker still runs its slice of the
executor in a single-threaded loop. CPU time remains the dominant cost on each
core, and the executor’s tight loops determine how efficiently that time is used.

This is where executor efficiency becomes not just relevant, but central.

## What gets in the way

PostgreSQL’s executor favors flexibility, but not CPU efficiency. Several design
choices lead to persistent overheads:

- The executor uses a **volcano-style model**, where each node calls the one
  below it to fetch one tuple at a time. This leads to frequent function calls,
  virtual dispatch, and poor instruction cache locality.  
- **Tuple deforming** is done one tuple at a time, one attribute at a time, with
  branches to handle nulls, alignment, and type-specific layout.  
- **Expression evaluation** runs a linear sequence of opcodes per tuple. While
  this avoids tree recursion, it's still row-at-a-time: each tuple is processed
  in isolation, with no batching or loop fusion. Intermediate results are boxed
  into `Datum`s, and there's little opportunity for SIMD or CPU pipelining.  

In analytical queries where the system processes millions of tuples, these small
per-tuple inefficiencies add up — and CPU usage becomes visibly dominated by
executor logic, not I/O.

## Low-hanging improvements

There are realistic improvements possible within the current executor structure
— particularly in the areas of batching and loop efficiency.

### 1. Batching between `ExecScanExtended()` and the table AM

Today, `ExecScanExtended()` pulls one tuple at a time from the table access
method. That means one function call per tuple, one deform, one qual eval.

A straightforward improvement would be to allow AMs to return small batches of
tuples — initially as arrays of `TupleTableSlot`, each wrapping a `HeapTuple`.
This would preserve existing executor abstractions while enabling:

- Fewer function calls  
- Loop-based processing of quals and deforming  
- Better instruction cache behavior

In the longer term, this could evolve into a more compact representation such as
a `TupleBatch`, reducing redundancy like repeated `TupleDesc` or per-tuple metadata.

### 2. Batched tuple deforming

Tuple deforming today walks over attributes one by one, checking for nulls and
format flags for every attribute in every tuple. There's no batching, and no
tight loop over a group of rows.

Even without rewriting the deforming logic, a simple loop that performs
deforming over a batch of `TupleTableSlot`s (each wrapping a `HeapTuple`) can
improve CPU efficiency:

- Instruction cache locality improves as the same deforming code is reused  
- Branch predictor accuracy increases when processing similar-shaped rows  
- Function call overhead is amortized across the batch

This baseline change can bring measurable improvements even before more complex
optimizations like SIMD or offset precomputation are introduced.

## Final notes

Executor performance matters because:

- It helps **OLTP queries** stay lean and responsive, especially in
  high-throughput environments  
- It speeds up **OLAP workloads** where executor loops consume most CPU time  
- It improves **parallel and distributed execution**, since the executor runs
  inside every worker  
- It clears the way for **future improvements** — like vectorization — by
  removing architectural blockers  

No matter how smart the planner is or how fast the storage layer gets, the executor
still runs the actual computation. It's where the tuples are filtered, joined,
aggregated, and returned — and it's still CPU-bound work.
